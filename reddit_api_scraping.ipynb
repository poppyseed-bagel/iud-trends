{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7470c821",
   "metadata": {},
   "source": [
    "## Reddit API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a2d98",
   "metadata": {},
   "source": [
    "This notebook displays the code that was used to extract Reddit posts data from the Reddit API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a33e6049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c02b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing private API keys and Reddit login information\n",
    "with open('reddit_keys.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Definining credentials\n",
    "client = data['reddit_api']['CLIENT_ID']\n",
    "secret = data['reddit_api']['SECRET_ID']\n",
    "reddit_data = data['reddit_data']\n",
    "auth = requests.auth.HTTPBasicAuth(client, secret)\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "\n",
    "# Gaining access to API\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                   auth=auth, data=reddit_data, headers=headers)\n",
    "TOKEN = res.json()['access_token']\n",
    "headers = {**headers, **{'Authorization': f'bearer {TOKEN}'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d13a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating reddit posts data file\n",
    "new_posts = pd.DataFrame()\n",
    "all_posts = pd.DataFrame()\n",
    "hot_posts = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c6bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddits = ['birthcontrol', 'IUD', 'TwoXChromosomes']\n",
    "categories = ['top', 'new', 'hot']\n",
    "prefix = 't3_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27c7fe3",
   "metadata": {},
   "source": [
    "As we loop through posts from new, hot and top pages of the r/BirthControl, r/IUD and r/TwoXChromosomes, we pull only the columns that we know we will need. This reduces unnecessary cleaning and data storing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f02c80",
   "metadata": {},
   "source": [
    "As we pull the data from the three subreddits, there is a strong possibility that we contravene the Reddit API thresholds. To ensure that we do not overwhelm the API limitations we use a time delay to respect these rules. When we meet an error with the pull we move onto the next loop by placing the code in a try/except statement.\n",
    "\n",
    "Whilst pulling relevant columns we complete some data transformations so that we do not unnecessarily iterate through all the data at another stage.\n",
    "* The UTC date stamp is transformed into a datetime object using the datetime package as we pull it in.\n",
    "* We remove the '/n' objects in the text box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77d03336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull data from Reddit API and store\n",
    "\n",
    "for s in subreddits:\n",
    "    for c in categories:\n",
    "        counter = 0\n",
    "        for i in range(20):\n",
    "            try:\n",
    "                # Calling API for relevant url without after parameter only for first run\n",
    "                if counter == 0:\n",
    "                    request = requests.get(f'https://oauth.reddit.com/r/{s}/{c}', headers=headers,\n",
    "                                           params={'limit': '100'})\n",
    "\n",
    "                # Reading in data from API\n",
    "                ls = []\n",
    "                for post in request.json()['data']['children']:\n",
    "                    # Taking only the columns that are relevant\n",
    "                    new_dict = {\n",
    "                        'subreddit': post['data']['subreddit'],\n",
    "                        'title': post['data']['title'],\n",
    "                        'text': post['data']['selftext'].strip(),\n",
    "                        'upvote_ratio' : post['data']['upvote_ratio'],\n",
    "                        'score' : post['data']['score'],\n",
    "                        'flair' : post['data']['link_flair_text'],\n",
    "                        'datetime' : pd.to_datetime(datetime.fromtimestamp(post['data']['created_utc'])),\n",
    "                        'num_comments' : post['data']['num_comments'], \n",
    "                        'id': post['data']['id']}\n",
    "                    ls.append(new_dict)\n",
    "\n",
    "                # Adding new posts to all posts directory\n",
    "                new_posts = pd.DataFrame(ls)\n",
    "                all_posts = pd.concat([all_posts, new_posts], axis=0)\n",
    "\n",
    "                # Updating fullname id to pull next 100 \n",
    "                idx = new_posts['id'].index.stop\n",
    "                last_id = new_posts['id'][idx - 1]\n",
    "                fullname = prefix + last_id\n",
    "\n",
    "                # Reset API GET call to include new fullname id\n",
    "                request = requests.get(f'https://oauth.reddit.com/r/{s}/{c}', headers=headers,\n",
    "                                       params={'limit': '100', 'after': fullname})\n",
    "\n",
    "                # Using time delay command to respect API request limits\n",
    "                time.sleep(2)\n",
    "                counter += 1\n",
    "\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4e77d",
   "metadata": {},
   "source": [
    "Save raw dataset for further data cleaning and exploratory analysis in the main project file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adbf71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_posts.to_csv('reddit_posts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
